# Overfitting and Underfitting

## Topics covered in today's module
* Overfitting
* Underfitting
* Data preparation
* Reducing network capacity

## Main takeaways from doing today's assignment

- **Data Cleaning:** Lowercasing, emojis, punctuation, stopwords, tokenization, and lemmatization are important steps in cleaning text data.

- **Underfitting:** This occurs when a model is too simple to capture the complexity of the data. Reducing the number of layers or neurons in a model, or removing activation functions, can lead to underfitting.

- **Reducing Underfitting:** Increase model complexity, decrease regularization strength, and train the model longer to reduce underfitting.

- **Overfitting:** This happens when a model learns the training data too well, including its noise, and performs poorly on unseen data.

- **Reducing Overfitting:** Apply regularization methods (like L1/L2 regularization, dropout, and early stopping), increase the amount of training data, use data augmentation techniques, and simplify the model to reduce overfitting.

- **Balance between Underfitting and Overfitting:** It's crucial to find the right balance between model complexity and model generalization to prevent both underfitting and overfitting. This often requires careful tuning and experimentation.

## Challenging, interesting, or exciting aspects of today's assignment
I found tokenization of text for training models very interesting and it was a new concept for me. This section allowed me to add yet another tool to my toolbox to use when developing models.

## Additional resources used 
[Text Cleaning article](https://monkeylearn.com/blog/text-cleaning/)


[Regularization Article](https://www.einfochips.com/blog/regularization-make-your-machine-learning-algorithms-learn-not-memorize/)
