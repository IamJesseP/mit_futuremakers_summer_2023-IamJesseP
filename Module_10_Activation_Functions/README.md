# Activation Functions

## Topics covered in today's module
* Introduction to Sigmoid, Tanh, ReLU
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
* Dying ReLU problem
* Advanced activation functions: Swish, GeLU, SeLU

## Main takeaways from doing today's assignment
* Activation functions, like Sigmoid, Tanh, ReLU, Leaky ReLU, and ELU, play an important role in neural networks. They introduce non-linearity and help the network learn complex patterns.
* The Sigmoid and Tanh functions map their inputs to a specific range between 0 and 1 or -1 and 1 respectively.
* ReLU is a popular activation function that outputs the input directly if positive, otherwise, it outputs zero. This function can sometimes cause the "dying ReLU" problem where neurons output zero and stop learning.
* Leaky ReLU is a variant of ReLU that has a small positive slope for negative inputs, which can keep the neurons from dying.
* ELU, or Exponential Linear Unit, is another type of activation function that helps alleviate the vanishing gradient problem and tends to converge cost to zero faster, producing more accurate results.


## Challenging, interesting, or exciting aspects of today's assignment
I understand the role of activation functions in neural networks, I may have to revisit the role of weights and how they connect to the big picture of neural networks.

